{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "179.991633 seconds (1.46 G allocations: 181.288 GiB, 6.43% gc time, 4.34% compilation time)\n",
      "Epoch 1 done. Training Accuracy: 89.44, Test Loss: 0.19917944982336788, Test Accuracy: 93.41\n",
      "172.298950 seconds (1.45 G allocations: 180.540 GiB, 6.77% gc time)\n"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "include(\"ConvolutionModule.jl\")\n",
    "include(\"PoolingModule.jl\")\n",
    "include(\"FlattenModule.jl\")\n",
    "include(\"DenseModule.jl\")\n",
    "\n",
    "include(\"MNISTDataLoader.jl\")\n",
    "include(\"LossAndAccuracy.jl\")\n",
    "include(\"NetworkHandlers.jl\")\n",
    "\n",
    "using .ConvolutionModule, .PoolingModule, .MNISTDataLoader, .FlattenModule, .DenseModule\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_features, train_labels = MNISTDataLoader.load_data(:train)\n",
    "train_x, train_y = MNISTDataLoader.preprocess_data(train_features, train_labels; one_hot=true)\n",
    "\n",
    "# for i in eachindex(axes(train_x, 4))\n",
    "#     input = train_x[:, :, :, i]\n",
    "#     (height, width, channels) = size(input)\n",
    "#     if height != 28 || width != 28 || channels != 1\n",
    "#         println(\"Input $i: height=$height, width=$width, channels=$channels\")\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# Load and preprocess test data\n",
    "test_features, test_labels = MNISTDataLoader.load_data(:test)\n",
    "test_x, test_y = MNISTDataLoader.preprocess_data(test_features, test_labels; one_hot=true)\n",
    "\n",
    "# Create batches\n",
    "batch_size = 100\n",
    "train_data = MNISTDataLoader.batch_data((train_x, train_y), batch_size; shuffle=true)\n",
    "# input_image = Float64.(input_image)\n",
    "\n",
    "sample_input = train_x[:, :, :, 1]\n",
    "\n",
    "# Initialize layers\n",
    "conv_layer1 = ConvolutionModule.init_conv_layer(3, 3, 1, 6, 1, 0, 3697631579, 28, 28, 1)\n",
    "pool_layer1 = PoolingModule.init_pool_layer(2, 2, 2, 26, 26, 6)\n",
    "conv_layer2 = ConvolutionModule.init_conv_layer(3, 3, 6, 16, 1, 0, 3731614026, 13, 13, 6)\n",
    "pool_layer2 = PoolingModule.init_pool_layer(2, 2, 2, 11, 11, 16)\n",
    "flatten_layer = FlattenModule.FlattenLayer()\n",
    "dense_layer1 = DenseModule.init_dense_layer(400, 84, DenseModule.relu, DenseModule.relu_grad, 4172219205)\n",
    "dense_layer2 = DenseModule.init_dense_layer(84, 10, DenseModule.identity, DenseModule.identity_grad, 3762133366)\n",
    "\n",
    "# Workaround because of namespaces...\n",
    "function backward_pass_master(network, grad_loss)\n",
    "    for layer in reverse(network)\n",
    "        if isa(layer, ConvolutionModule.ConvLayer)\n",
    "            grad_loss = ConvolutionModule.backward_pass(layer, grad_loss)\n",
    "\n",
    "        elseif isa(layer, PoolingModule.MaxPoolLayer)\n",
    "            grad_loss = PoolingModule.backward_pass(layer, grad_loss)\n",
    "\n",
    "        elseif isa(layer, DenseModule.DenseLayer)\n",
    "            grad_loss = DenseModule.backward_pass(layer, grad_loss)\n",
    "\n",
    "        elseif isa(layer, FlattenModule.FlattenLayer)\n",
    "            grad_loss = FlattenModule.backward_pass(layer, grad_loss)\n",
    "        else\n",
    "            println(\"No backward pass defined for layer type $(typeof(layer))\")\n",
    "        end\n",
    "    end\n",
    "    return grad_loss\n",
    "end\n",
    "\n",
    "function update_weights(network, learning_rate)\n",
    "    for layer in reverse(network)\n",
    "        if isa(layer, DenseModule.DenseLayer) || isa(layer, ConvolutionModule.ConvLayer)\n",
    "            # println(\"Type of layer \", typeof(layer))\n",
    "\n",
    "            # println(\"Previous weights grad \", layer.grad_weights[1:2, :])\n",
    "\n",
    "            layer.grad_weights ./= batch_size\n",
    "            layer.grad_biases ./= batch_size\n",
    "            # println(\"Normalized weights grad \", layer.grad_weights[1:2, :])\n",
    "\n",
    "            # println(\"Previous weights \", layer.weights[1:2, :])\n",
    "\n",
    "            layer.weights .-= learning_rate * layer.grad_weights\n",
    "            layer.biases .-= learning_rate * layer.grad_biases\n",
    "\n",
    "\n",
    "\n",
    "            # println(\"Updated weights \", layer.weights[1:2, :])\n",
    "\n",
    "            fill!(layer.grad_weights, 0)\n",
    "            fill!(layer.grad_biases, 0)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function to evaluate the model on test data\n",
    "function evaluate_model(network, test_x, test_y)\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_samples = size(test_x, 4)\n",
    "\n",
    "    for i in 1:num_samples\n",
    "        input = test_x[:, :, :, i]\n",
    "        target = test_y[:, i]\n",
    "\n",
    "        # Forward pass\n",
    "        output = NetworkHandlers.forward_pass_master(network, input)\n",
    "\n",
    "        # Calculate loss and accuracy\n",
    "        loss, accuracy, _ = LossAndAccuracy.loss_and_accuracy(output, target)\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "    end\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / num_samples\n",
    "    avg_accuracy = total_accuracy / num_samples\n",
    "    return avg_loss, avg_accuracy\n",
    "end\n",
    "\n",
    "# Assemble the network\n",
    "network = (conv_layer1, pool_layer1, conv_layer2, pool_layer2, flatten_layer, dense_layer1, dense_layer2)\n",
    "\n",
    "using .NetworkHandlers, .LossAndAccuracy\n",
    "epochs = 3\n",
    "training_step = 0.50\n",
    "\n",
    "println(\"Starting training...\")\n",
    "\n",
    "plot_loss = Float64[]\n",
    "batch_loss = 0.0\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    accumulated_accuracy_epoch = 0.0\n",
    "    accumulated_accuracy_batch = 0.0\n",
    "    \n",
    "    indices = eachindex(axes(train_x, 4))\n",
    "    @time begin\n",
    "        for i in indices\n",
    "            input = train_x[:, :, :, i]\n",
    "            target = train_y[:, i]\n",
    "\n",
    "            output = NetworkHandlers.forward_pass_master(network, input)\n",
    "            \n",
    "            loss, accuracy, grad_loss = LossAndAccuracy.loss_and_accuracy(output, target)\n",
    "            accumulated_accuracy_epoch += accuracy\n",
    "            accumulated_accuracy_batch += accuracy\n",
    "            batch_loss += loss\n",
    "            # if i % 100 == 0\n",
    "            #     println(\"Loss: \", loss)\n",
    "            #     println(\"Accuracy: \", round(accumulated_accuracy_batch / 100, digits=2))\n",
    "            #     accumulated_accuracy_batch = 0.0\n",
    "            # end\n",
    "\n",
    "            # if i % 10000 == 0\n",
    "            #     println(\"i \", i)\n",
    "            # end\n",
    "\n",
    "            backward_pass_master(network, grad_loss)\n",
    "            \n",
    "            if i % batch_size == 0\n",
    "                plot_loss = push!(plot_loss, batch_loss / batch_size)\n",
    "                batch_loss = 0.0\n",
    "                update_weights(network, training_step)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    # println(\"Epoch $(epoch) done\")\n",
    "    # println(\"Accuracy: \", round(accumulated_accuracy_epoch / size(train_x, 4), digits=2))\n",
    "    # accumulated_accuracy_epoch = 0.0\n",
    "\n",
    "    test_loss, test_accuracy = evaluate_model(network, test_x, test_y)\n",
    "    println(\"Epoch $(epoch) done. Training Accuracy: $(round(accumulated_accuracy_epoch / size(train_x, 4), digits=2)), Test Loss: $test_loss, Test Accuracy: $test_accuracy\")\n",
    "\n",
    "    # Update weights at the end of each epoch\n",
    "    update_weights(network, training_step)\n",
    "end\n",
    "\n",
    "# Plot\n",
    "using Plots\n",
    "\n",
    "plot(plot_loss, xlabel=\"Batch\", ylabel=\"Loss\", title=\"Loss over batches\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
