{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Utils.jl\")\n",
    "include(\"CNNm.jl\")\n",
    "include(\"SwimmingPool.jl\")\n",
    "include(\"Dense.jl\")\n",
    "\n",
    "# Define hyperparameters\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize weights and biases\n",
    "# These should be adjusted according to the dimensions expected by each layer\n",
    "kernel1 = randn(3, 3, 1, 6)  # Conv1 weights\n",
    "bias1 = zeros(6)             # Conv1 biases\n",
    "kernel2 = randn(3, 3, 6, 16) # Conv2 weights\n",
    "bias2 = zeros(16)            # Conv2 biases\n",
    "w1 = randn(576, 84)          # Dense layer 1 weights\n",
    "b1 = zeros(84)               # Dense layer 1 biases\n",
    "w2 = randn(84, 10)           # Dense layer 2 weights\n",
    "b2 = zeros(10);               # Dense layer 2 biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `train_data` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `train_data` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ f:\\GitHubRepository\\ADJuliaCNN\\ADJuliaCNN\\mikolaj_main.ipynb:3"
     ]
    }
   ],
   "source": [
    "# Main model training loop\n",
    "for epoch in 1:epochs\n",
    "    for (input, target) in loader_m(train_data, batch_size)\n",
    "        # Forward pass\n",
    "        x = conv_m(input, kernel1, bias1)\n",
    "        x = max_pool_m(x, (2, 2))\n",
    "        x = relu(x)\n",
    "        x = conv_m(x, kernel2, bias2)\n",
    "        x = max_pool_m(x, (2, 2))\n",
    "        x = relu(x)\n",
    "        x = flatten_m(x, :, size(x, 4))  # Flatten\n",
    "        x = dense_layer(x, w1, b1)\n",
    "        x = relu(x)\n",
    "        x = dense_layer(x, w2, b2)\n",
    "        predictions = softmaxior(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = cross_entropy(predictions, target)\n",
    "\n",
    "        # Backward pass (Reverse-mode AD)\n",
    "        gradients = reverse_mode_autodiff(loss_function, [kernel1, bias1, kernel2, bias2, w1, b1, w2, b2])\n",
    "\n",
    "        # Update weights and biases\n",
    "        kernel1 -= learning_rate * gradients[:kernel1]\n",
    "        bias1 -= learning_rate * gradients[:bias1]\n",
    "        kernel2 -= learning_rate * gradients[:kernel2]\n",
    "        bias2 -= learning_rate * gradients[:bias2]\n",
    "        w1 -= learning_rate * gradients[:w1]\n",
    "        b1 -= learning_rate * gradients[:b1]\n",
    "        w2 -= learning_rate * gradients[:w2]\n",
    "        b2 -= learning_rate * gradients[:b2]\n",
    "    end\n",
    "\n",
    "    # Evaluate model performance on test data every epoch\n",
    "    test_loss, test_accuracy = evaluate_model(test_data)\n",
    "    println(\"Epoch $epoch: Test Loss: $test_loss, Test Accuracy: $test_accuracy%\")\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
